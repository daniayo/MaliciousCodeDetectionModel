{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "405c9654-8e35-4082-8572-1e76ab132d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# PE, Ngram 데이터 로드\n",
    "malware_pe = pd.read_csv('malware_pe.csv', on_bad_lines='skip')\n",
    "normal_pe = pd.read_csv('normal_pe.csv' ,on_bad_lines='skip')\n",
    "ngram = pd.read_csv('ngram.csv')\n",
    "\n",
    "# 데이터 결합 (예: 악성 코드와 정상 코드 결합)\n",
    "X_pe = pd.concat([malware_pe, normal_pe], axis=0)\n",
    "y_pe = np.concatenate([np.ones(len(malware_pe)), np.zeros(len(normal_pe))])\n",
    "\n",
    "# Ngram 데이터 결합\n",
    "X_ngram = ngram.values\n",
    "y_ngram = np.concatenate([np.ones(len(malware_pe)), np.zeros(len(normal_pe))])\n",
    "\n",
    "# 이미지 데이터 로드 (이미지 크기를 64x64로 리사이즈)\n",
    "def load_images(image_dir, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for img_name in os.listdir(image_dir):\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (64, 64))  # 64x64 크기로 리사이즈\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "malware_images, y_images = load_images('/home/ubuntu/images/malware', 1)\n",
    "normal_images, _ = load_images('/home/ubuntu/images/normal', 0)\n",
    "\n",
    "X_images = np.concatenate([malware_images, normal_images], axis=0)\n",
    "y_images = np.concatenate([y_images, _], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "23d0a495-1f37-43aa-8b9b-288371fb7919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PE 데이터셋 컬럼: Index(['e_cblp', 'e_cp', 'e_cparhdr', 'e_maxalloc', 'e_sp', 'e_lfanew',\n",
      "       'NumberOfSections', 'CreationYear', 'FH_char0', 'FH_char1',\n",
      "       ...\n",
      "       'packer_.gfids', 'packer_.tls', 'packer_0', 'packer_PAGER32R',\n",
      "       'packer_type_.00cfg', 'packer_type_0', 'packer_type_PAGER32R',\n",
      "       'E_text_INITDATA', 'E_data_PAGEDATA', 'E_file_.reloc'],\n",
      "      dtype='object', length=312)\n",
      "Ngram 데이터셋 컬럼: Index(['mov mov mov mov', 'add add add add', 'int3 int3 int3 int3',\n",
      "       'push push push push', 'push push push call', 'mov mov mov call',\n",
      "       'mov mov call push', 'nop nop nop nop', 'push push call mov',\n",
      "       'mov mov call mov',\n",
      "       ...\n",
      "       'MD5_fb09af4f6edf6335d2778e42f1344bfd',\n",
      "       'MD5_fc2ff2a09f884114b62c36cdcb730356',\n",
      "       'MD5_fc9f896933b6123abebb21c8476448ec',\n",
      "       'MD5_fd30acc7a696c32f661b33668e73bf7b',\n",
      "       'MD5_fd442c307bc454d3930eaf6ec878fd36',\n",
      "       'MD5_febba1a2aefeece75f8d29aac8baf7e3',\n",
      "       'MD5_ff328a71371993ed57b6a52d94cde746',\n",
      "       'MD5_ff431011d7e84dae9de5eb25c4c7eb28',\n",
      "       'MD5_ff4bca27edbb839e459e2de9bf360607',\n",
      "       'MD5_ffa637abd482b5e7d3fb75182f43f080'],\n",
      "      dtype='object', length=1267)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PE 데이터셋 로드\n",
    "malware_pe = pd.read_csv('malware_pe.csv', on_bad_lines='skip')\n",
    "normal_pe = pd.read_csv('normal_pe.csv', on_bad_lines='skip')\n",
    "\n",
    "# PE 데이터셋에서 'filename'과 'MD5' 컬럼 제거\n",
    "malware_pe = malware_pe.drop(columns=['filename', 'MD5'], errors='ignore')\n",
    "normal_pe = normal_pe.drop(columns=['filename', 'MD5'], errors='ignore')\n",
    "\n",
    "# PE 데이터셋에 대해 One-Hot Encoding 수행\n",
    "malware_pe_encoded = pd.get_dummies(malware_pe)\n",
    "normal_pe_encoded = pd.get_dummies(normal_pe)\n",
    "\n",
    "# One-Hot Encoding된 PE 데이터셋 결합\n",
    "X_pe_encoded = pd.concat([malware_pe_encoded, normal_pe_encoded], axis=0)\n",
    "\n",
    "# 레이블 생성 (악성코드는 1, 정상은 0)\n",
    "y_pe = np.concatenate([np.ones(len(malware_pe)), np.zeros(len(normal_pe))])\n",
    "\n",
    "# Ngram 데이터셋 로드\n",
    "ngram = pd.read_csv('ngram.csv')\n",
    "\n",
    "# Ngram 데이터셋에 대해 One-Hot Encoding 수행\n",
    "ngram_encoded = pd.get_dummies(ngram)\n",
    "\n",
    "# Ngram 데이터셋 값 추출\n",
    "X_ngram_encoded = ngram_encoded.values\n",
    "y_ngram = np.concatenate([np.ones(len(malware_pe)), np.zeros(len(normal_pe))])\n",
    "\n",
    "# One-Hot Encoding 후 컬럼 확인\n",
    "print(\"PE 데이터셋 컬럼:\", X_pe_encoded.columns)\n",
    "print(\"Ngram 데이터셋 컬럼:\", ngram_encoded.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "89e79a6b-d61e-4476-929d-9b097c5b9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# PE 데이터셋 분할\n",
    "X_pe_train, X_pe_test, y_pe_train, y_pe_test = train_test_split(X_pe_encoded, y_pe, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ngram 데이터셋 분할\n",
    "# X_ngram_encoded의 길이에 맞게 y_ngram을 생성합니다.\n",
    "# y_ngram의 길이가 X_ngram_encoded의 길이에 맞지 않으면 오류가 발생하므로 정확히 맞춰야 합니다.\n",
    "num_samples = len(X_ngram_encoded)\n",
    "y_ngram = np.concatenate([np.ones(num_samples // 2), np.zeros(num_samples - num_samples // 2)])\n",
    "\n",
    "# train_test_split을 사용하여 Ngram 데이터를 분할합니다.\n",
    "X_ngram_train, X_ngram_test, y_ngram_train, y_ngram_test = train_test_split(X_ngram_encoded, y_ngram, test_size=0.2, random_state=42)\n",
    "\n",
    "# 이미지 데이터셋 분할\n",
    "X_images_train, X_images_test, y_images_train, y_images_test = train_test_split(X_images, y_images, test_size=0.2, random_state=42)\n",
    "\n",
    "# 표준화: PE 데이터셋\n",
    "scaler_pe = StandardScaler()\n",
    "X_pe_train_scaled = scaler_pe.fit_transform(X_pe_train)  # 학습 데이터로 fit\n",
    "X_pe_test_scaled = scaler_pe.transform(X_pe_test)  # 테스트 데이터는 transform만 수행\n",
    "\n",
    "# 표준화: Ngram 데이터셋\n",
    "scaler_ngram = StandardScaler()\n",
    "X_ngram_train_scaled = scaler_ngram.fit_transform(X_ngram_train)\n",
    "X_ngram_test_scaled = scaler_ngram.transform(X_ngram_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "127038cc-04ce-40f7-b536-aa741ccf4c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PE 데이터 예측:\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        71\n",
      "         1.0       1.00      1.00      1.00        58\n",
      "\n",
      "    accuracy                           1.00       129\n",
      "   macro avg       1.00      1.00      1.00       129\n",
      "weighted avg       1.00      1.00      1.00       129\n",
      "\n",
      "Ngram 데이터 예측:\n",
      "Accuracy: 0.6752136752136753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.58      1.00      0.73        52\n",
      "         1.0       1.00      0.42      0.59        65\n",
      "\n",
      "    accuracy                           0.68       117\n",
      "   macro avg       0.79      0.71      0.66       117\n",
      "weighted avg       0.81      0.68      0.65       117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# NaN 처리: SimpleImputer 사용\n",
    "imputer = SimpleImputer(strategy='mean')  # NaN 값을 평균으로 대체\n",
    "\n",
    "# PE 데이터 NaN 처리\n",
    "X_pe_train = imputer.fit_transform(X_pe_train)\n",
    "X_pe_test = imputer.transform(X_pe_test)\n",
    "\n",
    "# Ngram 데이터 NaN 처리\n",
    "X_ngram_train = imputer.fit_transform(X_ngram_train)\n",
    "X_ngram_test = imputer.transform(X_ngram_test)\n",
    "\n",
    "# 표준화: PE 데이터\n",
    "scaler_pe = StandardScaler()\n",
    "X_pe_train_scaled = scaler_pe.fit_transform(X_pe_train)\n",
    "X_pe_test_scaled = scaler_pe.transform(X_pe_test)\n",
    "\n",
    "# 표준화: Ngram 데이터\n",
    "scaler_ngram = StandardScaler()\n",
    "X_ngram_train_scaled = scaler_ngram.fit_transform(X_ngram_train)\n",
    "X_ngram_test_scaled = scaler_ngram.transform(X_ngram_test)\n",
    "\n",
    "# SVM 모델 학습: PE 데이터\n",
    "svm_model_pe = SVC(random_state=42)\n",
    "svm_model_pe.fit(X_pe_train_scaled, y_pe_train)\n",
    "\n",
    "# SVM 모델 학습: Ngram 데이터\n",
    "svm_model_ngram = SVC(random_state=42)\n",
    "svm_model_ngram.fit(X_ngram_train_scaled, y_ngram_train)\n",
    "\n",
    "# PE 데이터 예측 및 성능 평가\n",
    "y_pe_pred = svm_model_pe.predict(X_pe_test_scaled)\n",
    "print(\"PE 데이터 예측:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_pe_test, y_pe_pred))\n",
    "print(classification_report(y_pe_test, y_pe_pred))\n",
    "\n",
    "# Ngram 데이터 예측 및 성능 평가\n",
    "y_ngram_pred = svm_model_ngram.predict(X_ngram_test_scaled)\n",
    "print(\"Ngram 데이터 예측:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_ngram_test, y_ngram_pred))\n",
    "print(classification_report(y_ngram_test, y_ngram_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ed8bbbc0-72db-442c-828f-3ac3fa790af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PE 데이터 성능:\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        71\n",
      "         1.0       1.00      1.00      1.00        58\n",
      "\n",
      "    accuracy                           1.00       129\n",
      "   macro avg       1.00      1.00      1.00       129\n",
      "weighted avg       1.00      1.00      1.00       129\n",
      "\n",
      "Ngram 데이터 성능:\n",
      "Accuracy: 0.8205128205128205\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      1.00      0.83        52\n",
      "         1.0       1.00      0.68      0.81        65\n",
      "\n",
      "    accuracy                           0.82       117\n",
      "   macro avg       0.86      0.84      0.82       117\n",
      "weighted avg       0.87      0.82      0.82       117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# NaN 값 처리: SimpleImputer 사용\n",
    "imputer = SimpleImputer(strategy='mean')  # NaN 값을 평균으로 대체\n",
    "\n",
    "# PE 데이터 NaN 처리\n",
    "X_pe_train = imputer.fit_transform(X_pe_train)\n",
    "X_pe_test = imputer.transform(X_pe_test)\n",
    "\n",
    "# Ngram 데이터 NaN 처리\n",
    "X_ngram_train = imputer.fit_transform(X_ngram_train)\n",
    "X_ngram_test = imputer.transform(X_ngram_test)\n",
    "\n",
    "# 표준화: PE 데이터\n",
    "scaler_pe = StandardScaler()\n",
    "X_pe_train_scaled = scaler_pe.fit_transform(X_pe_train)\n",
    "X_pe_test_scaled = scaler_pe.transform(X_pe_test)\n",
    "\n",
    "# 표준화: Ngram 데이터\n",
    "scaler_ngram = StandardScaler()\n",
    "X_ngram_train_scaled = scaler_ngram.fit_transform(X_ngram_train)\n",
    "X_ngram_test_scaled = scaler_ngram.transform(X_ngram_test)\n",
    "\n",
    "# RandomForest 모델 학습: PE 데이터\n",
    "rf_model_pe = RandomForestClassifier(random_state=42)\n",
    "rf_model_pe.fit(X_pe_train_scaled, y_pe_train)\n",
    "\n",
    "# RandomForest 모델 학습: Ngram 데이터\n",
    "rf_model_ngram = RandomForestClassifier(random_state=42)\n",
    "rf_model_ngram.fit(X_ngram_train_scaled, y_ngram_train)\n",
    "\n",
    "# PE 데이터 예측 및 성능 평가\n",
    "y_pe_pred = rf_model_pe.predict(X_pe_test_scaled)\n",
    "print(\"PE 데이터 성능:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_pe_test, y_pe_pred))\n",
    "print(classification_report(y_pe_test, y_pe_pred))\n",
    "\n",
    "# Ngram 데이터 예측 및 성능 평가\n",
    "y_ngram_pred = rf_model_ngram.predict(X_ngram_test_scaled)\n",
    "print(\"Ngram 데이터 성능:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_ngram_test, y_ngram_pred))\n",
    "print(classification_report(y_ngram_test, y_ngram_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a63c7864-afd4-4ed3-aa5a-75dc9f37cad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PE 데이터 성능:\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        71\n",
      "         1.0       1.00      1.00      1.00        58\n",
      "\n",
      "    accuracy                           1.00       129\n",
      "   macro avg       1.00      1.00      1.00       129\n",
      "weighted avg       1.00      1.00      1.00       129\n",
      "\n",
      "Ngram 데이터 성능:\n",
      "Accuracy: 0.5641025641025641\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.02      0.04        52\n",
      "         1.0       0.56      1.00      0.72        65\n",
      "\n",
      "    accuracy                           0.56       117\n",
      "   macro avg       0.78      0.51      0.38       117\n",
      "weighted avg       0.76      0.56      0.42       117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# NaN 값 처리: SimpleImputer 사용\n",
    "imputer = SimpleImputer(strategy='mean')  # NaN 값을 평균으로 대체\n",
    "\n",
    "# PE 데이터 NaN 처리\n",
    "X_pe_train = imputer.fit_transform(X_pe_train)\n",
    "X_pe_test = imputer.transform(X_pe_test)\n",
    "\n",
    "# Ngram 데이터 NaN 처리\n",
    "X_ngram_train = imputer.fit_transform(X_ngram_train)\n",
    "X_ngram_test = imputer.transform(X_ngram_test)\n",
    "\n",
    "# 표준화: PE 데이터\n",
    "scaler_pe = StandardScaler()\n",
    "X_pe_train_scaled = scaler_pe.fit_transform(X_pe_train)\n",
    "X_pe_test_scaled = scaler_pe.transform(X_pe_test)\n",
    "\n",
    "# 표준화: Ngram 데이터\n",
    "scaler_ngram = StandardScaler()\n",
    "X_ngram_train_scaled = scaler_ngram.fit_transform(X_ngram_train)\n",
    "X_ngram_test_scaled = scaler_ngram.transform(X_ngram_test)\n",
    "\n",
    "# Naive Bayes 모델 학습: PE 데이터\n",
    "nb_model_pe = GaussianNB()\n",
    "nb_model_pe.fit(X_pe_train_scaled, y_pe_train)\n",
    "\n",
    "# Naive Bayes 모델 학습: Ngram 데이터\n",
    "nb_model_ngram = GaussianNB()\n",
    "nb_model_ngram.fit(X_ngram_train_scaled, y_ngram_train)\n",
    "\n",
    "# PE 데이터 예측 및 성능 평가\n",
    "y_pe_pred = nb_model_pe.predict(X_pe_test_scaled)\n",
    "print(\"PE 데이터 성능:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_pe_test, y_pe_pred))\n",
    "print(classification_report(y_pe_test, y_pe_pred))\n",
    "\n",
    "# Ngram 데이터 예측 및 성능 평가\n",
    "y_ngram_pred = nb_model_ngram.predict(X_ngram_test_scaled)\n",
    "print(\"Ngram 데이터 성능:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_ngram_test, y_ngram_pred))\n",
    "print(classification_report(y_ngram_test, y_ngram_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "357bb2a3-8e45-46c3-a46b-2cdb95d2c6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "17/17 [==============================] - 1s 3ms/step - loss: 0.3649 - accuracy: 0.8932\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0895 - accuracy: 0.9864\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.9981\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9961\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 1s 5ms/step - loss: 0.7391 - accuracy: 0.5622\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4176 - accuracy: 0.8369\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2431 - accuracy: 0.9442\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1251 - accuracy: 0.9828\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9936\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0328 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "PE 데이터 성능:\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        71\n",
      "         1.0       1.00      1.00      1.00        58\n",
      "\n",
      "    accuracy                           1.00       129\n",
      "   macro avg       1.00      1.00      1.00       129\n",
      "weighted avg       1.00      1.00      1.00       129\n",
      "\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "Ngram 데이터 성능:\n",
      "Accuracy: 0.6153846153846154\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      1.00      0.70        52\n",
      "         1.0       1.00      0.31      0.47        65\n",
      "\n",
      "    accuracy                           0.62       117\n",
      "   macro avg       0.77      0.65      0.58       117\n",
      "weighted avg       0.79      0.62      0.57       117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# PE 데이터 분할\n",
    "X_pe_train, X_pe_test, y_pe_train, y_pe_test = train_test_split(X_pe_encoded, y_pe, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ngram 데이터 분할\n",
    "num_samples = len(X_ngram_encoded)\n",
    "y_ngram = np.concatenate([np.ones(num_samples // 2), np.zeros(num_samples - num_samples // 2)])\n",
    "X_ngram_train, X_ngram_test, y_ngram_train, y_ngram_test = train_test_split(X_ngram_encoded, y_ngram, test_size=0.2, random_state=42)\n",
    "\n",
    "# NaN 값 처리: SimpleImputer 사용\n",
    "imputer = SimpleImputer(strategy='mean')  # NaN 값을 평균으로 대체\n",
    "\n",
    "# PE 데이터 NaN 처리\n",
    "X_pe_train = imputer.fit_transform(X_pe_train)\n",
    "X_pe_test = imputer.transform(X_pe_test)\n",
    "\n",
    "# Ngram 데이터 NaN 처리\n",
    "X_ngram_train = imputer.fit_transform(X_ngram_train)\n",
    "X_ngram_test = imputer.transform(X_ngram_test)\n",
    "\n",
    "# 표준화: PE 데이터\n",
    "scaler_pe = StandardScaler()\n",
    "X_pe_train_scaled = scaler_pe.fit_transform(X_pe_train)\n",
    "X_pe_test_scaled = scaler_pe.transform(X_pe_test)\n",
    "\n",
    "# 표준화: Ngram 데이터\n",
    "scaler_ngram = StandardScaler()\n",
    "X_ngram_train_scaled = scaler_ngram.fit_transform(X_ngram_train)\n",
    "X_ngram_test_scaled = scaler_ngram.transform(X_ngram_test)\n",
    "\n",
    "# DNN 모델 학습: PE 데이터\n",
    "dnn_model_pe = Sequential()\n",
    "dnn_model_pe.add(Dense(128, input_dim=X_pe_train_scaled.shape[1], activation='relu'))\n",
    "dnn_model_pe.add(Dropout(0.2))\n",
    "dnn_model_pe.add(Dense(64, activation='relu'))\n",
    "dnn_model_pe.add(Dense(1, activation='sigmoid'))  # 이진 분류\n",
    "\n",
    "dnn_model_pe.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "dnn_model_pe.fit(X_pe_train_scaled, y_pe_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# DNN 모델 학습: Ngram 데이터\n",
    "dnn_model_ngram = Sequential()\n",
    "dnn_model_ngram.add(Dense(128, input_dim=X_ngram_train_scaled.shape[1], activation='relu'))\n",
    "dnn_model_ngram.add(Dropout(0.2))\n",
    "dnn_model_ngram.add(Dense(64, activation='relu'))\n",
    "dnn_model_ngram.add(Dense(1, activation='sigmoid'))  # 이진 분류\n",
    "\n",
    "dnn_model_ngram.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "dnn_model_ngram.fit(X_ngram_train_scaled, y_ngram_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# PE 데이터 예측 및 성능 평가\n",
    "y_pe_pred = dnn_model_pe.predict(X_pe_test_scaled)\n",
    "y_pe_pred = (y_pe_pred > 0.5)  # 확률을 0.5 이상이면 1로 분류\n",
    "\n",
    "print(\"PE 데이터 성능:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_pe_test, y_pe_pred))\n",
    "print(classification_report(y_pe_test, y_pe_pred))\n",
    "\n",
    "# Ngram 데이터 예측 및 성능 평가\n",
    "y_ngram_pred = dnn_model_ngram.predict(X_ngram_test_scaled)\n",
    "y_ngram_pred = (y_ngram_pred > 0.5)  # 확률을 0.5 이상이면 1로 분류\n",
    "\n",
    "print(\"Ngram 데이터 성능:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_ngram_test, y_ngram_pred))\n",
    "print(classification_report(y_ngram_test, y_ngram_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "aa66b02e-02a5-4a86-9fac-bc95d5ce91f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 4s 112ms/step - loss: 48.0179 - accuracy: 0.5333\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 4s 114ms/step - loss: 0.4722 - accuracy: 0.8167\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 4s 111ms/step - loss: 0.3755 - accuracy: 0.8598\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 4s 112ms/step - loss: 0.3150 - accuracy: 0.8745\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 4s 113ms/step - loss: 0.2481 - accuracy: 0.8951\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 4s 126ms/step - loss: 0.2035 - accuracy: 0.9196\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 4s 111ms/step - loss: 0.1947 - accuracy: 0.9294\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 4s 109ms/step - loss: 0.1586 - accuracy: 0.9343\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 4s 112ms/step - loss: 0.1377 - accuracy: 0.9451\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 4s 110ms/step - loss: 0.1230 - accuracy: 0.9520\n",
      "8/8 [==============================] - 0s 46ms/step\n",
      "CNN Accuracy: 0.91015625\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "cnn_model.add(MaxPooling2D((2, 2)))\n",
    "cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D((2, 2)))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "cnn_model.fit(X_images_train, y_images_train, epochs=10, batch_size=32, verbose=1)\n",
    "y_images_pred_cnn = (cnn_model.predict(X_images_test) > 0.5).astype(\"int32\")\n",
    "print(f\"CNN Accuracy: {accuracy_score(y_images_test, y_images_pred_cnn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d51af4e1-b771-4586-b9ac-d9c8fe7eb6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 14ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "8/8 [==============================] - 0s 40ms/step\n",
      "          Model  PE Accuracy  Ngram Accuracy  Images Accuracy\n",
      "0           SVM          1.0        0.675214         0.000000\n",
      "1  RandomForest          1.0        0.820513         0.000000\n",
      "2    NaiveBayes          1.0        0.564103         0.000000\n",
      "3           DNN          1.0        0.615385         0.000000\n",
      "4           CNN          0.0        0.000000         0.910156\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 예측: DNN 모델 (PE)\n",
    "y_pe_pred_dnn = dnn_model_pe.predict(X_pe_test_scaled)\n",
    "y_pe_pred_dnn = (y_pe_pred_dnn > 0.5)  # 확률을 이진 값으로 변환\n",
    "\n",
    "# 예측: DNN 모델 (Ngram)\n",
    "y_ngram_pred_dnn = dnn_model_ngram.predict(X_ngram_test_scaled)\n",
    "y_ngram_pred_dnn = (y_ngram_pred_dnn > 0.5)  # 확률을 이진 값으로 변환\n",
    "\n",
    "# 예측: CNN 모델 (이미지)\n",
    "y_images_pred_cnn = cnn_model.predict(X_images_test)\n",
    "y_images_pred_cnn = (y_images_pred_cnn > 0.5).astype(\"int32\")  # 확률을 이진 값으로 변환\n",
    "\n",
    "# 모델별 성능 결과\n",
    "results = {\n",
    "    \"Model\": [\"SVM\", \"RandomForest\", \"NaiveBayes\", \"DNN\", \"CNN\"],\n",
    "    \"PE Accuracy\": [\n",
    "        accuracy_score(y_pe_test, svm_model_pe.predict(X_pe_test_scaled)),\n",
    "        accuracy_score(y_pe_test, rf_model_pe.predict(X_pe_test_scaled)),\n",
    "        accuracy_score(y_pe_test, nb_model_pe.predict(X_pe_test_scaled)),\n",
    "        accuracy_score(y_pe_test, y_pe_pred_dnn),  # DNN 모델 PE 정확도\n",
    "        0  # CNN은 PE 데이터에서 학습하지 않음\n",
    "    ],\n",
    "    \"Ngram Accuracy\": [\n",
    "        accuracy_score(y_ngram_test, svm_model_ngram.predict(X_ngram_test_scaled)),\n",
    "        accuracy_score(y_ngram_test, rf_model_ngram.predict(X_ngram_test_scaled)),\n",
    "        accuracy_score(y_ngram_test, nb_model_ngram.predict(X_ngram_test_scaled)),\n",
    "        accuracy_score(y_ngram_test, y_ngram_pred_dnn),  # DNN 모델 Ngram 정확도\n",
    "        0  # CNN은 Ngram 데이터에서 학습하지 않음\n",
    "    ],\n",
    "    \"Images Accuracy\": [\n",
    "        0,  # SVM은 이미지 데이터에서 학습하지 않음\n",
    "        0,  # RandomForest는 이미지 데이터에서 학습하지 않음\n",
    "        0,  # NaiveBayes는 이미지 데이터에서 학습하지 않음\n",
    "        0,  # DNN은 이미지 데이터에서 학습하지 않음\n",
    "        accuracy_score(y_images_test, y_images_pred_cnn)  # CNN 모델 이미지 정확도\n",
    "    ]\n",
    "}\n",
    "\n",
    "# DataFrame으로 변환\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# 결과 출력\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a9c71-533d-45b9-8b06-12f1f203999b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
